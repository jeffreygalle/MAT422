{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOQ+7a0vMgpjwza+mvUZUV+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffreygalle/MAT422/blob/main/Homework1_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.3.1. QR decomposition\n",
        "\n",
        "QR decomposition: breaks down a matrix A into two matrices, an orthogonal matrix Q; and an upper triangular matrix R.\n",
        "\n",
        "If A is an m x n matrix (where m ≥ n): A = QR\n",
        "where:\n",
        "\n",
        "*   Q us an m x n matrix with orthonormal columns (i.e. Q^t*Q = I)\n",
        "*   R is an n x n upper triangular matrix\n",
        "\n",
        "\n",
        "\n",
        "## Gram-Schmidt algorithm to QR Decomposition\n",
        "\n",
        "\n",
        "1.   Given a set of linear indepdent vectors {a1, a2, ..., an}, these form the **columns of matrix A**.\n",
        "2.   Orthogonalization: With Gram-Schmidt we can transform the set {a1, a2, ..., an} into an orthonormal basis {q1, q2, ..., qn}.\n",
        "*   Normalize: Set q1 = a1/||a1||\n",
        "*   Iterate: For each ai, subtract its projection onto each of the already computed q vectors. This makes it orthogonal. Then normalize those orthogonal values to obtain the next q.\n",
        "*   Build R: R is made from the coefficents from the projections (the dot product of q, i)\n",
        "Together these form the **columns of the Q matrix**.\n",
        "3. Building R: The coefficents that represent each ai as a combination of the orthonormal vectors q are the **upper triangular matrix R**.\n",
        "\n",
        "\n",
        "   Example\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Consider the matrix \\( A \\) given by:\n",
        "Given the matrix A:\n",
        "\n",
        "A = \\begin{bmatrix} 1 & 2 \\\\ 2 & 3 \\end{bmatrix}\n",
        "\n",
        "\n",
        "Decompose this matrix into orthogonal (Q) and upper triangular (R) matrices so that: A = QR\n",
        "\n",
        "Let a1 = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}  and a2 = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix} \\\n",
        "\n",
        "Normalize a1 to get q1: q1 = a1/||a1|| = \\begin{bmatrix} 1/sqrt(5) \\\\ 2/sqrt(5) \\end{bmatrix}\n",
        "Project a2 onto q1 and subtract:\n",
        "proj_q1(a2) = (q1 * a2) * q1\n",
        "Normalize u2 to get q2: q2 = u2/||u2||\n",
        "\n",
        "The matrix R is formed using the dot product:\n",
        "R = \\begin{bmatrix} \\mathbf{q}_1 \\cdot \\mathbf{a}_1 & \\mathbf{q}_1 \\cdot \\mathbf{a}_2 \\\\ 0 & \\mathbf{q}_2 \\cdot \\mathbf{a}_2 \\end{bmatrix}\n"
      ],
      "metadata": {
        "id": "WXY4yVLPbx2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Matrix A same as above\n",
        "A = np.array([[1, 2],\n",
        "              [2, 3]])\n",
        "\n",
        "# Perform QR decomposition\n",
        "Q, R = np.linalg.qr(A)\n",
        "\n",
        "print(\"Matrix A:\")\n",
        "print(A)\n",
        "print(\"\\nMatrix Q:\")\n",
        "print(Q)\n",
        "print(\"\\nMatrix R:\")\n",
        "print(R)\n",
        "\n",
        "print(\"\\ Matrix QR:\")\n",
        "print(np.dot(Q, R))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipdcSCLdcJ7G",
        "outputId": "9dacc476-364c-445a-85c5-c2666bef945e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix A:\n",
            "[[1 2]\n",
            " [2 3]]\n",
            "\n",
            "Matrix Q:\n",
            "[[-0.4472136  -0.89442719]\n",
            " [-0.89442719  0.4472136 ]]\n",
            "\n",
            "Matrix R:\n",
            "[[-2.23606798 -3.57770876]\n",
            " [ 0.         -0.4472136 ]]\n",
            "\\ Matrix QR:\n",
            "[[1. 2.]\n",
            " [2. 3.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.3.2. Least-squares Problems\n",
        "\n",
        "\n",
        "Least-squares Problem Defintion: Method to find the best-fitting solution to an over-determined system (more equations than unknowns (n > m).)\n",
        "\n",
        "\n",
        "Goal: minimize the sum of the squares of the residuals, which are the differences between the observed values and the values predicted by the model.\n",
        "\n",
        "Goal part-2: minimize the objective function ||Ax - b||^2, where ||.|| denotes the Euclidean norm. The solution to this minimization problem is given by the equation AᵀAx = Aᵀb, assuming A has linearly independent columns. The matrix AᵀA is guaranteed to be invertible, and the solution can be found as:\n",
        "x = (AᵀA)⁻¹Aᵀb."
      ],
      "metadata": {
        "id": "q_0_vFLPiwD2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.3.3. Linear regression\n",
        "\n",
        "In linear regression the goal is to find an affine function (short summary: takes an input, multiplies it by a constant, adds another constant to it, and produces an output) that best fits a given set of input data points {(xi, yi)}. The input data points consist of a set of xi values, where each xi is a d-dimensional vector (xi1, ..., xid)T, and their corresponding yi values. The affine function has the form f(xi) = β0 + β1xi1 + ... + βd xid, where β0, β1, ..., βd are the coefficients of the function.\n",
        "\n",
        "How: use Least Squares. Least squares minimizes the sum of the squared differences between the predicted values and the actual values of the yi. The coefficients β0, β1, ..., βd are estimated to minimize this sum of squared differences, giving us the best-fitting affine function.\n",
        "\n",
        "Once the coefficients are found, use the linear regression model to make predictions for new input data points"
      ],
      "metadata": {
        "id": "nLjrJsi-je_I"
      }
    }
  ]
}