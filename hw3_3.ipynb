{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNi6Welm5auvnJ94ZPUx/DF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffreygalle/MAT422/blob/main/hw3_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.3.1 Necessary and Suﬀicent Conditions of Local Minimizers\n",
        "\n",
        "**Definition Global minimizer:** The set of points within the domain of a function where the function achieves its minimum value acorss its entire domain.\n",
        "\n",
        "**Definition Local minimizer:** The set of points within the domain of a function where the function achieves its minimum value in a neighborhood around that point.\n",
        "\n",
        "\n",
        "Example 3.3.9: for f(x) = x^3 we find ∇f(0) = 0 and f\"(0) ≥ 0. Note: though the conditions are satisfied, x = 0 is not a local minimizer since f(−δ) < f(0) for small δ > 0.\n",
        "\n",
        "\n",
        "**Theorem (Second-Order Sufficient Condition):**\n",
        "\n",
        "For a function f: R^d → R that is twice continuously differentiable:\n",
        "\n",
        "\n",
        "*  if ∇f(x0) and the Hessian Hf(x0) is positive, then x0 is a strict local minimizer\n",
        "*   This makes sure a local minimum exists if the curvature of f is positive around x0.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uyVK4HgmB4AY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.3.2. Convexity and global minimizers\n",
        "\n",
        "**Definition 3.3.11 (Convex Set):** A set S is convex if it contains all the line segements that connect any two points in S. For any points x, y in S and any scalar λ with 0 ≤ λ ≤ 1, the point λx + (1 - λ)y is also in S.\n",
        "\n",
        "**Definition 3.3.13 (Convex Function):** A function is convex when any two points pn its domain, the function value at the midpoint of the segment joining those points is less than or equal to the midpoint of the function values at the two points.\n",
        "\n",
        "**Properties:**\n",
        "\n",
        "\n",
        "\n",
        "*   Every local minimum is also global minimum.\n",
        "\n",
        "*   Unique global minima.\n",
        "\n",
        "*   Smooth and have well-defined derivatives everywhere.\n",
        "\n"
      ],
      "metadata": {
        "id": "wtjj37WPFMS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.3.2.2 Global Minimizers of Convex Functions\n",
        "\n",
        "Definiton: a point in the function domain where  function attains minimum values. The point at which the convex function reaches its lowest value."
      ],
      "metadata": {
        "id": "iG6PnoeOFveA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.3.3 Gradient Descent\n",
        "\n",
        "\n",
        "\n",
        "**Definition 3.3.3 (Descent Direction):** The direction in which the objective function decreases. Given a function f(x) the descent direction is a direction in which we can move from the current point x to reduce the value of f(x).\n",
        "\n",
        "\n",
        "Summary: Gradient descent is an optimization algorithm used to minimize a differentiable function by iteratively moving in the direction of the negative gradient of the function."
      ],
      "metadata": {
        "id": "UG8BriMmFz2d"
      }
    }
  ]
}