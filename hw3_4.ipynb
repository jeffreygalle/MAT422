{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPeqtve6I4rxm+1oKzJnw7D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffreygalle/MAT422/blob/main/hw3_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.4. Logistic regression\n",
        "\n",
        "Logistic regression is primarily used for binary classification problems.\n",
        "\n",
        "\n",
        "Goal: Model the probability that a binary outcome is “1” given a set of input features.\n",
        "\n",
        "Training data:\n",
        "\n",
        "*   Use training data (α_i, b_i) for i=1,...,n, where α_i ∈ R^d are feature vectors and b_i ∈ {0,1} are binary labels.\n",
        "*   In matrix form, A ∈ R^(n×d) contains all feature vectors as rows, and b ∈ {0,1}^n contains the labels\n",
        "\n",
        "Model:\n",
        "\n",
        "\n",
        "1.   P(label \"1\") given feature α is modeled as p(α; x) = σ(α^T x), where σ(t) = 1/(1+e^(-t)) is the sigmoid function.\n",
        "2.   Term α^T x is regression output that is then inserted into the range [0,1] by the sigmoid.\n",
        "\n",
        "Logit func:\n",
        "\n",
        "\n",
        "*   Definition: log(p(α; x)/(1 - p(α; x))) = α^T x.\n",
        "*   There is direct linear relationship between the log-odds and the features.\n",
        "\n",
        "Loss func:\n",
        "\n",
        "\n",
        "*   Goal: Maximize likelihood of observing the given labels under model assumptions, i.e. find the best parameters x.\n",
        "*   Use negative log-likelihood and normalization, optimization reduces to minimizing loss.\n",
        "\n",
        "\n",
        "Gradient Descent:\n",
        "\n",
        "*   Purpose: To find the parameter vector x.\n",
        "*   The derivative of the sigmoid σ'(t) = σ(t)(1 - σ(t)) is used to compute the gradient of the loss. That gradient provides update rule for parameters to reduce loss.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AJYhoU5Q9Dtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Small data with 2 features and a binary label\n",
        "np.random.seed(0)\n",
        "X = np.random.randn(100, 2)\n",
        "\n",
        " # label 1 if sum of features > 0, else label 0\n",
        "y = []\n",
        "for i in range(len(X)):\n",
        "    if X[i, 0] + X[i, 1] > 0:\n",
        "        y.append(1)\n",
        "    else:\n",
        "        y.append(0)\n",
        "\n",
        "y = np.array(y)\n",
        "\n",
        "\n",
        "# divide dataset into training and testing sets (20% testing, 80% training)\n",
        "train_size = 0.8\n",
        "test_size = 1 - train_size\n",
        "random_seed = 42\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=test_size, random_state=random_seed\n",
        ")\n",
        "\n",
        "print(f\"training set: {len(X_train)}\")\n",
        "print(f\"testing set: {len(X_test)}\")\n",
        "\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# predict on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3M_Np9WO9lDV",
        "outputId": "0c7df188-1898-495a-fe2b-a6d94851657b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training set: 80\n",
            "testing set: 20\n",
            "accuracy: 1.0\n"
          ]
        }
      ]
    }
  ]
}