{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMJ1YTrY3+e6vG9Wv9n/AJe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffreygalle/MAT422/blob/main/hw_1_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.4.1. Singular value decomposition\n",
        "The Singular Value Decomposition (SVD) of a matrix is a factorization of the matrix into three matrices. SVD is a key component of principal components analysis (PCA) - a useful and powerful dimension reduction technique.\n",
        "\n",
        "\n",
        "The SVD of a m x n matrix A can be expressed in terms of the factorization of A into the product of the three matrices; A = U * Σ * V^T\n",
        "\n",
        "Note that the columns of U and V are orthonormal, and the matrix Σ is diagonal with real positive entries.\n",
        "\n",
        "U: m x m matrix of the orthonormal eigenvectors of A*A^T.\n",
        "\n",
        "V^T: the transpose of an n x n matrix containing the orthonormal eigenvectos of A^T * A.\n",
        "\n",
        "\n",
        "Σ: a diagonal matrix with r elements equal to the root of the positive eigenvalues of A*A^T or A^T*A\n"
      ],
      "metadata": {
        "id": "tMVtms0KKVCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example in Python\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "A = np.array([[4, 0, 2], [1, 3, 0]])\n",
        "\n",
        "# Perform SVD .svd on matrix A\n",
        "U, S, Vt = np.linalg.svd(A)\n",
        "\n",
        "# convert S into a diagonal matrix form\n",
        "# because matrix S is returned as a 1D array of singular values\n",
        "# convert it into a diagonal matrix and match the dimensions of the original matrix A.\n",
        "\n",
        "# Create zeroed matrix of the same size as A with 2 rows\n",
        "S_matrix = np.zeros((2, 3))\n",
        "\n",
        "np.fill_diagonal(S_matrix, S)\n",
        "\n",
        "# reconstructing matrix A\n",
        "A_reconstructed = U @ S_matrix @ Vt\n",
        "\n",
        "print(\"Matrix A:\")\n",
        "print(A)\n",
        "print(\"\\nMatrix U:\")\n",
        "print(U)\n",
        "print(\"\\nSingular values (S):\")\n",
        "print(S)\n",
        "print(\"\\nMatrix Vt:\")\n",
        "print(Vt)\n",
        "print(\"\\nReconstruction of matrix A:\")\n",
        "print(A_reconstructed)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itIIksF3P-eq",
        "outputId": "dd52810f-cd26-46ac-8f4f-5f9152ec74a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix A:\n",
            "[[4 0 2]\n",
            " [1 3 0]]\n",
            "\n",
            "Matrix U:\n",
            "[[-0.94362832 -0.33100694]\n",
            " [-0.33100694  0.94362832]]\n",
            "\n",
            "Singular Values (S):\n",
            "[4.62635107 2.93204293]\n",
            "\n",
            "Matrix Vt:\n",
            "[[-0.88742081 -0.2146445  -0.40793632]\n",
            " [-0.1297387   0.96549915 -0.22578588]\n",
            " [-0.44232587  0.14744196  0.88465174]]\n",
            "\n",
            "Reconstructed Matrix A:\n",
            "[[4.00000000e+00 2.77574497e-16 2.00000000e+00]\n",
            " [1.00000000e+00 3.00000000e+00 6.00525422e-17]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.4.2. Low-rank matrix approximations\n",
        "\n",
        "Aim of Low rank matrix approximation: Technique that uses SVD to create a \"simplified\" version of a matrix but still preserving the matricies' essential information. Only the most significant singular values and their corresponding vectors are retained.\n",
        "\n",
        "The rank of a (m x n) matrix is determined by the number of linearly independent rows present in that matrix. Reducing the rank during reconstruction leads to creating a low-rank approximation of the original matrix.\n",
        "\n",
        "Low-rank approximations can reduce the dimensionality of high dimensional data, making it easier to store and process. [*source*: https://www.geeksforgeeks.org/eigenvector-computation-and-low-rank-approximations/ ]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jI4L7bQFSsFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "A = np.array([[3, 2, 2, 4],\n",
        "              [1, 3, 1, 1],\n",
        "              [0, 0, 0, 0],\n",
        "              [5, 6, 7, 8]])\n",
        "\n",
        "# Perform SVD with .svd on matrix A\n",
        "U, S, Vt = np.linalg.svd(A)\n",
        "\n",
        "\n",
        "# Similar as above convert singular values S into diag matrix\n",
        "S_matrix = np.zeros((U.shape[0], Vt.shape[0]))\n",
        "np.fill_diagonal(S_matrix, S)\n",
        "\n",
        "# set up the low-rank approximation by selecting only the top k components, here we\n",
        "# will use k =2, try k=3\n",
        "k = 2\n",
        "\n",
        "# keep only the top k components for U, S, Vt\n",
        "U_k = U[:, :k]\n",
        "S_k = S_matrix[:k, :k]\n",
        "Vt_k = Vt[:k, :]\n",
        "\n",
        "# ** Construct the rank-k approximation of A **\n",
        "A_approx = U_k @ S_k @ Vt_k\n",
        "\n",
        "print(\"Original matrix A:\")\n",
        "print(A)\n",
        "print(\"\\nTruncated singular values:\")\n",
        "print(S[:k])\n",
        "print(\"\\nApproximated matrix A (Rank = 2):\")\n",
        "print(A_approx)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3IjoMx_U4f1",
        "outputId": "3e4e2447-86d3-46f8-ec81-dfffc202a76e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original matrix A:\n",
            "[[3 2 2 4]\n",
            " [1 3 1 1]\n",
            " [0 0 0 0]\n",
            " [5 6 7 8]]\n",
            "\n",
            "Truncated singular values:\n",
            "[14.60384575  2.04699042]\n",
            "\n",
            "Approximated matrix A (Rank = 2):\n",
            "[[2.46691183 1.78742843 2.79851757 3.86020766]\n",
            " [0.72329351 2.8896619  1.41448115 0.92743893]\n",
            " [0.         0.         0.         0.        ]\n",
            " [5.28661471 6.11428905 6.57067725 8.07515931]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.4.3. Principal component analysis\n",
        "\n",
        "# 1.4.3.1 Covariance Matrix\n",
        "\n",
        "The covariance matrix summarizes the covariance (a measure of how much two random variables vary together) between pairs of variables in a dataset. The covariance matrix is also a square matrix.\n",
        "\n",
        "\n",
        "The covariance matrix is computed using Cov(X) = 1/(m-1) * X ^T\n",
        "where X is the centered data matrix.\n",
        "\n",
        "\n",
        "# 1.4.3.2 Principal component analysis\n",
        "\n",
        "Principal Component Analysis (PCA) is a technique used for dimensionality reduction, data compression, and feature extraction. It transforms a large set of data into a smaller one that still contains most of the original information.\n",
        "\n",
        "\n",
        "Steps in PCA [*source:* https://www.geeksforgeeks.org/principal-component-analysis-pca/ ]:\n",
        "\n",
        "1. Standardize the Data: Scale the data to have a mean of 0 and standard deviation of 1.\n",
        "2. Compute the Covariance Matrix: Calculate the covariance matrix to understand the relationships between variables.\n",
        "3. Compute Eigenvalues and Eigenvectors: Determine the eigenvectors and eigenvalues of the covariance matrix to identify the principal components.\n",
        "4. Select Principal Components: Choose the top k eigenvectors corresponding to the largest eigenvalues, which capture the most variance.\n",
        "5. Transform the Data: Project the data onto the selected principal components to obtain the reduced dataset.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YgXbMyYHV9lN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#  dataset with 3 variables each with 5 observations\n",
        "data = np.array([[2.5, 2.4, 3.2],\n",
        "                 [0.5, 0.7, 1.1],\n",
        "                 [2.2, 2.9, 3.5],\n",
        "                 [1.9, 2.2, 2.8],\n",
        "                 [3.1, 3.0, 3.6]])\n",
        "\n",
        "# Convert DataFrame\n",
        "df = pd.DataFrame(data, columns=['Feature1', 'Feature2', 'Feature3'])\n",
        "\n",
        "# Step 1: Standardize the data\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "# apply the PCA\n",
        "pca = PCA(n_components=2)  # We choose 2 components for simplicity\n",
        "principal_components = pca.fit_transform(scaled_data)\n",
        "\n",
        "#  principal components dataFrame\n",
        "pca_df = pd.DataFrame(principal_components, columns=['Principal Component 1', 'Principal Component 2'])\n",
        "\n",
        "print(\"Original Data:\\n\")\n",
        "print(df)\n",
        "print(\"\\n\\nPrincipal Components:\\n\")\n",
        "print(pca_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zHYQp0QbMEb",
        "outputId": "119934c1-3f11-45a5-a2be-296768af19ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data:\n",
            "\n",
            "   Feature1  Feature2  Feature3\n",
            "0       2.5       2.4       3.2\n",
            "1       0.5       0.7       1.1\n",
            "2       2.2       2.9       3.5\n",
            "3       1.9       2.2       2.8\n",
            "4       3.1       3.0       3.6\n",
            "\n",
            "\n",
            "Principal Components:\n",
            "\n",
            "   Principal Component 1  Principal Component 2\n",
            "0              -0.644572              -0.211700\n",
            "1               3.203124              -0.058527\n",
            "2              -0.988777               0.465536\n",
            "3               0.145770               0.094697\n",
            "4              -1.715544              -0.290006\n"
          ]
        }
      ]
    }
  ]
}